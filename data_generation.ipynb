{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Dataset Generation\n",
    "\n",
    "Generation of datasets for multiclass classification and regression cases"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Multiclass classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd"
   ]
  },
  {
   "source": [
    "Because we want to know which are the real informative features, we generate the data without shuffeling.\n",
    "Then we safe the the names of the features with x for informative and y for the rest. Random state is set to 42 so the datasets will always be the same."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset 1\n",
    "20000 samples with 100 features with 15 informative features, 10 classes. \n",
    "There are no redundant or repeated features,\n",
    "all classes have the same weights.\n",
    "\"\"\"\n",
    "data1 = make_classification(n_samples=20000,n_features=100, n_classes=10, n_informative=15, shuffle=False, random_state=42)\n",
    "\n",
    "# create feature names\n",
    "columns = []\n",
    "for i in range(100):\n",
    "    if i < 15:\n",
    "        columns.append(\"x\"+str(i))\n",
    "    else:\n",
    "        columns.append(\"y\"+str(i))\n",
    "\n",
    "# create dataframe\n",
    "data1_df = pd.DataFrame(data1[0], columns=columns)\n",
    "\n",
    "# shuffle features\n",
    "data1_df = data1_df.sample(frac=1, axis=1)\n",
    "\n",
    "# add label\n",
    "data1_df[\"label\"] = data1[1]\n",
    "\n",
    "# shuffle rows\n",
    "data1_df = data1_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# split into train and test\n",
    "data1_df_train = data1_df.iloc[:14999,]\n",
    "data1_df_test = data1_df.iloc[15000:,]\n",
    "\n",
    "#write data\n",
    "data1_df_train.to_csv(\"dataset_1_train.csv\", header=False,index=False)\n",
    "data1_df_test.to_csv(\"dataset_1_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Dataset 2\n",
    "50000 samples with 500 features with 80 informative features, 6 classes. \n",
    "There are 50 redundant and repeated features,\n",
    "all classes have the same weights.\n",
    "\"\"\"\n",
    "data2 = make_classification(n_samples=50000,n_features=500, n_classes=6, n_informative=80, n_redundant=50, n_repeated=50, shuffle=False, random_state=42)\n",
    "\n",
    "# create feature names\n",
    "columns = []\n",
    "for i in range(500):\n",
    "    if i < 80:\n",
    "        columns.append(\"x\"+str(i))\n",
    "    else:\n",
    "        columns.append(\"y\"+str(i))\n",
    "\n",
    "# create dataframe\n",
    "data2_df = pd.DataFrame(data2[0], columns=columns)\n",
    "\n",
    "# shuffle features\n",
    "data2_df = data2_df.sample(frac=1, axis=1)\n",
    "\n",
    "# add label\n",
    "data2_df[\"label\"] = data2[1]\n",
    "\n",
    "# shuffle rows\n",
    "data2_df = data2_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# split into train and test\n",
    "data2_df_train = data2_df.iloc[:39999,]\n",
    "data2_df_test = data2_df.iloc[40000:,]\n",
    "\n",
    "#write data\n",
    "data2_df_train.to_csv(\"dataset_2_train.csv\", header=False,index=False)\n",
    "data2_df_test.to_csv(\"dataset_2_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset 3\n",
    "5000 samples with 250 features with 20 informative features, 8 classes. \n",
    "There are 50 redundant and repeated features,\n",
    "all classes have the different weights.\n",
    "\n",
    "\"\"\"\n",
    "weights = [0.1, 0.05, 0.15, 0.2, 0.025, 0.125, 0.075, 0.275]\n",
    "data3 = make_classification(n_samples=5000,n_features=250, n_classes=8, n_informative=20, n_redundant=50, n_repeated=50, shuffle=False, random_state=42, weights=weights)\n",
    "\n",
    "# create feature names\n",
    "columns = []\n",
    "for i in range(250):\n",
    "    if i < 20:\n",
    "        columns.append(\"x\"+str(i))\n",
    "    else:\n",
    "        columns.append(\"y\"+str(i))\n",
    "\n",
    "# create dataframe\n",
    "data3_df = pd.DataFrame(data3[0], columns=columns)\n",
    "\n",
    "# shuffle features\n",
    "data3_df = data3_df.sample(frac=1, axis=1)\n",
    "\n",
    "# add label\n",
    "data3_df[\"label\"] = data3[1]\n",
    "\n",
    "# shuffle rows\n",
    "data3_df = data3_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# split into train and test\n",
    "data3_df_train = data3_df.iloc[:4199,]\n",
    "data3_df_test = data3_df.iloc[4200:,]\n",
    "\n",
    "#write data\n",
    "data3_df_train.to_csv(\"dataset_3_train.csv\", header=False,index=False)\n",
    "data3_df_test.to_csv(\"dataset_3_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset 4\n",
    "50000 samples with 200 features with 200 informative features, 10 classes. \n",
    "\"\"\"\n",
    "data4 = make_classification(n_samples=50000,n_features=210, n_classes=10, n_informative=200, random_state=42)\n",
    "\n",
    "\n",
    "# create dataframe\n",
    "data4_df = pd.DataFrame(data4[0])\n",
    "\n",
    "# add label\n",
    "data4_df[\"label\"] = data4[1]\n",
    "\n",
    "# shuffle rows\n",
    "data4_df = data4_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# split into train and test\n",
    "data4_df_train = data4_df.iloc[:44999,]\n",
    "data4_df_test = data4_df.iloc[45000:,]\n",
    "\n",
    "#write data\n",
    "data4_df_train.to_csv(\"dataset_4_train.csv\", header=False,index=False)\n",
    "data4_df_test.to_csv(\"dataset_4_test.csv\", index=False)"
   ]
  },
  {
   "source": [
    "### Regression Case"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "Here we can get the informative features via the coef parameter. So we don't need to shuffle the result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['x27', 'x30', 'x31', 'x33', 'x36', 'x37', 'x42', 'x43', 'x44', 'x47',\n       'x50', 'x62', 'x70', 'x71', 'x75', 'x83', 'x84', 'x92', 'x106', 'x111',\n       'x121', 'x127', 'x129', 'x130', 'x134'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataset1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data1 = make_regression(n_samples=20000, n_features=150, n_informative=25, coef=True, random_state=42)\n",
    "\n",
    "data1_df = pd.DataFrame(data1[0], columns = ['x'+str(i) for i in range(150)])\n",
    "\n",
    "data1_df_norm = pd.DataFrame(MinMaxScaler().fit_transform(data1_df))\n",
    "data1_df_norm.columns = data1_df.columns\n",
    "\n",
    "\n",
    "data1_df[\"y\"] = data1[1]\n",
    "data1_df_norm[\"y\"] = data1[1]\n",
    "\n",
    "informative_ftrs = data1_df.columns[np.where(data1[2] != 0)[0]]\n",
    "\n",
    "print(informative_ftrs)\n",
    "\n",
    "# shall labels also be scaled?\n",
    "scaler = MinMaxScaler()\n",
    "data1_df_norm = pd.DataFrame(scaler.fit_transform(data1_df))\n",
    "data1_df_norm.columns = data1_df.columns\n",
    "\n",
    "# split into train and test\n",
    "data1_df_train = data1_df.iloc[:16999,]\n",
    "data1_df_test = data1_df.iloc[17000:,]\n",
    "data1_df_norm_train = data1_df_norm.iloc[:16999,]\n",
    "data1_df_norm_test = data1_df_norm.iloc[17000:,]\n",
    "\n",
    "#write data\n",
    "data1_df_train.to_csv(\"dataset_1_train.csv\", header=False,index=False)\n",
    "data1_df_test.to_csv(\"dataset_1_test.csv\", index=False)\n",
    "data1_df_norm_train.to_csv(\"dataset_1_norm_train.csv\", header=False,index=False)\n",
    "data1_df_norm_test.to_csv(\"dataset_1_norm_test.csv\", index=False)"
   ]
  }
 ]
}