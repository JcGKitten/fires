{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Multiclass Case"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other test variables, which are self parameters\n",
    "# classes schould be given as int from 0 to k\n",
    "n_total_ftr = 0\n",
    "target_values = [1,2,3,4,5,6,7,8,9,0]\n",
    "mu = np.ones((n_total_ftr, len(target_values))) * 0\n",
    "sigma = np.ones((n_total_ftr, len(target_values))) * 1\n",
    "penalty_s = 0.01\n",
    "penalty_r = 0.01\n",
    "epochs = 1\n",
    "lr_mu = 0.01\n",
    "lr_sigma = 0.01\n",
    "monte_carlo = 5 #10000\n",
    "\n",
    "# create 3d array with all r for current observation \n",
    "# r^cl_j = r[l, j, c]\n",
    "#needs to be specified only if we got right model\n",
    "#r = np.random.randn(monte_carlo, n_total_ftr, len(target_values))\n",
    "\n",
    "# maybe set param amount_classes from target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __help_softmax(array_eta, class_eta):\n",
    "    \"\"\"\n",
    "        Calculates the softmax equation for given class etas and searched etas\n",
    "        :param array_eta: (np.ndarray) Array with etas of all classes\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __softmax(x,y): # needs self in model\n",
    "    \"\"\"\n",
    "        Update the distribution parameters mu and sigma by optimizing them in terms of the (log) likelihood.\n",
    "        Here we assume a Bernoulli distributed target variable. We use a Probit model as our base model.\n",
    "        This corresponds to the FIRES-GLM model in the paper.\n",
    "\n",
    "        :param x: (np.ndarray) Batch of observations (numeric values only, consider normalizing data for better results)\n",
    "        :param y: (np.ndarray) Batch of labels: type integer e.g. 1,2,3,4 usw\n",
    "     \"\"\"\n",
    "\n",
    "     for epoch in range(epochs): #changed to self.epoch in model\n",
    "         # Shuffle the observations\n",
    "         random_idx = np.random.permutation(len(y))\n",
    "         x = x[random_idx]\n",
    "         y = y[random_idx]\n",
    "\n",
    "         #maybe loop over all given observations\n",
    "         x = x[0]\n",
    "         y = y[0]\n",
    "         # Iterative update of mu and sigma\n",
    "         try:\n",
    "             # l number of samples, j features, c classes\n",
    "             # create 3d array with all r for current observation \n",
    "             # r^cl_j = r[l, j, c] lxjxc\n",
    "             r = np.random.randn(monte_carlo, n_total_ftr, len(target_values))\n",
    "             # we only change the psi for the actuall given class still need all classes of course\n",
    "             # r = np.random.randn(monte_carlo, n_total_ftr)\n",
    "\n",
    "             print(r.shape)\n",
    "             # calculate thetas for all samples and classes theta^cl_jt = theta[l,j,c]\n",
    "             # lxjxc\n",
    "             theta = r * sigma + mu\n",
    "\n",
    "             #calculate all the etas\n",
    "             eta = np.einsum(\"ljc,j->ljc\", theta, x) # multiply all ftr_cols with given ftr_vector x\n",
    "             eta = np.einsum(\"ljc->lc\", eta) #sum up all theta^cl_j * x_tj so we got l samples for all c classes\n",
    "             eta = np.exp(eta) # we only need them exp\n",
    "             eta_sum = np.einsum(\"lc->l\", eta) #sum up etas for the l samples\n",
    "             \n",
    "             #calculate softmax only for observed class\n",
    "             softmax_lh = (eta[:,y].T / eta_sum).T  #lxo o is amount of given observations\n",
    "             \n",
    "             marginal = np.einsum(\"lo->o\", softmax_lh) / monte_carlo # 1xy\n",
    "\n",
    "\n",
    "             # calculate derivatives nabla_mu, nabla_sigma must be handled better\n",
    "\n",
    "             #first calculate softmax dtheta\n",
    "\n",
    "             softmax_derivative = \n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for getting gaussnorm r ~ N(0,1) size samples x features\n",
    "test = np.random.randn(10000,100)\n",
    "print(test)\n",
    "print(test[1])\n",
    "print(test[9, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to calculate theta^cl_t\n",
    "print(np.array([1,2,3,4,5]) * np.array([2,3,4,5,6]) + np.ones(5))\n",
    "\n",
    "#for sampling create Matrix with all r monte_carlo X \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get theta at time t and for all featurs for class c is number of feature\n",
    "theta_c = sigma[:,c] * np.random.randn(total_features) + mu[:,c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 2,  4,  6],\n",
       "       [ 4,  8, 12],\n",
       "       [ 6, 12, 18],\n",
       "       [ 8, 16, 24]])"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "bla = np.array([1,2,3,4])\n",
    "np.sum(np.exp(bla))\n",
    "bla2 = np.array((2,4,6))\n",
    "np.einsum(\"i,j->ij\", bla, bla2) * bla2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 2.,  4.,  6.],\n",
       "       [ 4.,  8., 12.],\n",
       "       [ 6., 12., 18.],\n",
       "       [ 8., 16., 24.]])"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "(np.einsum(\"j,l->jl\", bla, bla2) * bla2) / bla2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1 1 1 1]\n [2 2 2 2]\n [3 3 3 3]]\n[[[1 2 3 4 5]\n  [1 2 3 4 5]\n  [1 2 3 4 5]\n  [1 2 3 4 5]]\n\n [[4 4 4 4 4]\n  [4 4 4 4 4]\n  [4 4 4 4 4]\n  [4 4 4 4 4]]\n\n [[9 9 9 9 9]\n  [9 9 9 9 9]\n  [9 9 9 9 9]\n  [9 9 9 9 9]]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[0.8, 1.2, 1.2, 0.8, 0. ],\n",
       "        [0.8, 1.2, 1.2, 0.8, 0. ],\n",
       "        [0.8, 1.2, 1.2, 0.8, 0. ],\n",
       "        [0.8, 1.2, 1.2, 0.8, 0. ]],\n",
       "\n",
       "       [[2.4, 2.4, 2.4, 2.4, 2.4],\n",
       "        [2.4, 2.4, 2.4, 2.4, 2.4],\n",
       "        [2.4, 2.4, 2.4, 2.4, 2.4],\n",
       "        [2.4, 2.4, 2.4, 2.4, 2.4]],\n",
       "\n",
       "       [[3.6, 3.6, 3.6, 3.6, 3.6],\n",
       "        [3.6, 3.6, 3.6, 3.6, 3.6],\n",
       "        [3.6, 3.6, 3.6, 3.6, 3.6],\n",
       "        [3.6, 3.6, 3.6, 3.6, 3.6]]])"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "# tests for derivative o = 3, j = 4, l = 5 entries\n",
    "X = np.array([[1,1,1,1], [2,2,2,2], [3,3,3,3]])\n",
    "print(X)\n",
    "eta = np.array([[1,2,3,4,5], [2,2,2,2,2], [3,3,3,3,3]])\n",
    "sums_eta = np.array((5,5,5,5,5))\n",
    "x_eta = np.einsum(\"oj,ol->ojl\", X, eta)\n",
    "print(x_eta)\n",
    "np.einsum(\"ojl,ol->ojl\", x_eta, (sums_eta-eta)) / sums_eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[4, 3, 2, 1, 0],\n",
       "       [3, 3, 3, 3, 3],\n",
       "       [2, 2, 2, 2, 2]])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "sums_eta - eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.random.randn(2,3,4) #monte, features, classes\n",
    "print(test.shape)\n",
    "print(\"----------\")\n",
    "print(test[1])\n",
    "print(\"-------\")\n",
    "print(test[1,1])\n",
    "print(\"--------\")\n",
    "print(test[1,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[(1,2),(3,4)], [(5,6),(7,8)]])\n",
    "ones = np.ones((2,2))\n",
    "twos = np.ones((2,2)) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[ 3.,  5.],\n",
       "        [ 7.,  9.]],\n",
       "\n",
       "       [[11., 13.],\n",
       "        [15., 17.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "test * twos + ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test[0]\n",
    "#print(test1)\n",
    "#(test1.T * np.array([1,2])).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1 2]\n [6 8]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "c = np.einsum(\"ij,i->ij\", test1, np.array((1,2)))\n",
    "print(c)\n",
    "c[0,1] #feature class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([4, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "np.einsum(\"ij->j\", test1) # for nabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([3, 7])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "np.einsum(\"ij->i\", test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[1, 2],\n",
       "        [3, 4]],\n",
       "\n",
       "       [[5, 6],\n",
       "        [7, 8]]])"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 4  6]\n [12 14]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "blub = np.einsum(\"ijk->ik\", test)\n",
    "print(blub) # -> blub[sample, class]\n",
    "blub[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(25).reshape(5,5)\n",
    "b = np.arange(5)\n",
    "a\n",
    "print(test)\n",
    "test2 = np.einsum(\"ijk,j->ijk\", test, np.array((1,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1. 2.]\n3.0\nresult:  [0.33333333 0.66666667]\n[3. 4.]\n7.0\nresult:  [0.42857143 0.57142857]\n[5. 6.]\n11.0\nresult:  [0.45454545 0.54545455]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.66666667],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.45454545, 0.54545455]])"
      ]
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "#test for softmax\n",
    "\n",
    "def fun_test(x):\n",
    "    print(x)\n",
    "    result = np.empty(len(x))\n",
    "    sum_all = np.sum(x)\n",
    "    print(sum_all)\n",
    "    for idx, entry in enumerate(x):\n",
    "        result[idx] = entry/sum_all\n",
    "    print(\"result: \", result)\n",
    "    return result\n",
    "\n",
    "\n",
    "fun_test_v = np.vectorize(fun_test)\n",
    "\n",
    "test_array = np.array([[1,2],[3,4],[5,6]], dtype=np.float)\n",
    "\n",
    "for idx, x in enumerate(test_array):\n",
    "    test_array[idx] = fun_test(x)\n",
    "\n",
    "test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "a = np.array((1,2,3,4,5,6))\n",
    "a / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[  2.71828183,   7.3890561 ,  20.08553692,  54.59815003,\n",
       "        148.4131591 ],\n",
       "       [  2.71828183,   7.3890561 ,  20.08553692,  54.59815003,\n",
       "        148.4131591 ],\n",
       "       [  2.71828183,   7.3890561 ,  20.08553692,  54.59815003,\n",
       "        148.4131591 ],\n",
       "       [  2.71828183,   7.3890561 ,  20.08553692,  54.59815003,\n",
       "        148.4131591 ]])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# test for simultaneous caclculations\n",
    "test_matrix = np.array([[1,2,3,4,5],\n",
    "                        [1,2,3,4,5],\n",
    "                        [1,2,3,4,5],\n",
    "                        [1,2,3,4,5]])\n",
    "test_matrix[:,[0,1,0,2,3]]\n",
    "test_matrix.T / np.array((1,2,3,4))\n",
    "np.exp(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "np.ones(4)"
   ]
  },
  {
   "source": [
    "\n",
    "### Further Questions/ Tasks\n",
    "- [x] softmax derivative\n",
    "- [x] read softmax glm implementation again\n",
    "- [x] check where class check happens\n",
    "- [] is simultaneous calculation for all given observetions possible\n",
    "- [] rewrite np.einsum indexes so they match l, j and c"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}