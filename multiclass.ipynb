{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other test variables, which are self parameters\n",
    "n_total_ftr = 0\n",
    "target_values = [1,2,3,4,5,6,7,8,9,0]\n",
    "mu = np.ones((n_total_ftr, len(target_values))) * 0\n",
    "sigma = np.ones((n_total_ftr, len(target_values))) * 1\n",
    "penalty_s = 0.01\n",
    "penalty_r = 0.01\n",
    "epochs = 1\n",
    "lr_mu = 0.01\n",
    "lr_sigma = 0.01\n",
    "monte_carlo = 5 #10000\n",
    "\n",
    "# create 3d array with all r for current observation \n",
    "# r^cl_j = r[l, j, c]\n",
    "#needs to be specified only if we got right model\n",
    "r = np.random.randn(monte_carlo, n_total_ftr, len(target_values))\n",
    "\n",
    "# maybe set param amount_classes from target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __help_softmax(array_eta):\n",
    "    \"\"\"\n",
    "        Calculates the softmax equation for given class etas and searched etas\n",
    "        :param array_eta: (np.ndarray) Array with etas of all classes\n",
    "    \"\"\"\n",
    "    result = np.empty(len(x))\n",
    "    sum_all = np.sum(x)\n",
    "    print(sum_all)\n",
    "    for idx, entry in enumerate(x):\n",
    "        result[idx] = entry/sum_all\n",
    "    print(\"result: \", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __softmax(x,y): # needs self in model\n",
    "    \"\"\"\n",
    "        Update the distribution parameters mu and sigma by optimizing them in terms of the (log) likelihood.\n",
    "        Here we assume a Bernoulli distributed target variable. We use a Probit model as our base model.\n",
    "        This corresponds to the FIRES-GLM model in the paper.\n",
    "\n",
    "        :param x: (np.ndarray) Batch of observations (numeric values only, consider normalizing data for better results)\n",
    "        :param y: (np.ndarray) Batch of labels: type integer e.g. 1,2,3,4 usw\n",
    "     \"\"\"\n",
    "\n",
    "     for epoch in range(epochs): #changed to self.epoch in model\n",
    "         # Shuffle the observations\n",
    "         random_idx = np.random.permutation(len(y))\n",
    "         x = x[random_idx]\n",
    "         y = y[random_idx]\n",
    "\n",
    "         #maybe loop over all given observations\n",
    "         x = x[0]\n",
    "         y = y[0]\n",
    "         # Iterative update of mu and sigma\n",
    "         try:\n",
    "             # create 3d array with all r for current observation \n",
    "             # r^cl_j = r[l, j, c]\n",
    "             # r_t = np.random.randn(monte_carlo, n_total_ftr, len(target_values))\n",
    "             print(r.shape)\n",
    "             # calculate thetas for all samples and classes theta^cl_jt = theta[l,j,c]\n",
    "             theta = r * sigma + mu \n",
    "\n",
    "             #calculate all the etas\n",
    "             eta = np.einsum(\"ijk,j->ijk\", theta, x) # multiply all ftr_cols with given ftr_vector x\n",
    "             eta = np.einsum(\"ijk->ik\", eta) #sum up all theta^cl_j * x_tj\n",
    "             \n",
    "             #calculate softmax \n",
    "             marginal_lh = np.empty(monte_carlo, len(target_values))\n",
    "             for idx, sample in enumerate(eta):\n",
    "                 marginal_lh[idx] = __help_softmax(sample)\n",
    "\n",
    "             # calculate derivatives nabla_mu, nabla_sigma must be handled better\n",
    "             nabla_mu = np.einsum(\"ij->j\", marginal_lh)/monte_carlo # add all sample marginal likelihoods devide by monte carlo\n",
    "\n",
    "             nabla_sigma = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for getting gaussnorm r ~ N(0,1) size samples x features\n",
    "test = np.random.randn(10000,100)\n",
    "print(test)\n",
    "print(test[1])\n",
    "print(test[9, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to calculate theta^cl_t\n",
    "print(np.array([1,2,3,4,5]) * np.array([2,3,4,5,6]) + np.ones(5))\n",
    "\n",
    "#for sampling create Matrix with all r monte_carlo X \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get theta at time t and for all featurs for class c is number of feature\n",
    "theta_c = sigma[:,c] * np.random.randn(total_features) + mu[:,c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = [1,2,3,4]\n",
    "np.sum(np.exp(bla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.random.randn(2,3,4) #monte, features, classes\n",
    "print(test.shape)\n",
    "print(\"----------\")\n",
    "print(test[1])\n",
    "print(\"-------\")\n",
    "print(test[1,1])\n",
    "print(\"--------\")\n",
    "print(test[1,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[(1,2),(3,4)], [(5,6),(7,8)]])\n",
    "ones = np.ones((2,2))\n",
    "twos = np.ones((2,2)) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test * twos + ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test[0]\n",
    "#print(test1)\n",
    "#(test1.T * np.array([1,2])).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.einsum(\"ij,i->ij\", test1, np.array((1,2)))\n",
    "print(c)\n",
    "c[0,1] #feature class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2., 3.])"
      ]
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "source": [
    "np.einsum(\"ij->j\", test1) /2 # for nabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[1, 2],\n",
       "        [3, 4]],\n",
       "\n",
       "       [[5, 6],\n",
       "        [7, 8]]])"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 4  6]\n [12 14]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "blub = np.einsum(\"ijk->ik\", test)\n",
    "print(blub) # -> blub[sample, class]\n",
    "blub[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(25).reshape(5,5)\n",
    "b = np.arange(5)\n",
    "a\n",
    "print(test)\n",
    "test2 = np.einsum(\"ijk,j->ijk\", test, np.array((1,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1. 2.]\n3.0\nresult:  [0.33333333 0.66666667]\n[3. 4.]\n7.0\nresult:  [0.42857143 0.57142857]\n[5. 6.]\n11.0\nresult:  [0.45454545 0.54545455]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.66666667],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.45454545, 0.54545455]])"
      ]
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "#test for softmax\n",
    "\n",
    "def fun_test(x):\n",
    "    print(x)\n",
    "    result = np.empty(len(x))\n",
    "    sum_all = np.sum(x)\n",
    "    print(sum_all)\n",
    "    for idx, entry in enumerate(x):\n",
    "        result[idx] = entry/sum_all\n",
    "    print(\"result: \", result)\n",
    "    return result\n",
    "\n",
    "\n",
    "fun_test_v = np.vectorize(fun_test)\n",
    "\n",
    "test_array = np.array([[1,2],[3,4],[5,6]], dtype=np.float)\n",
    "\n",
    "for idx, x in enumerate(test_array):\n",
    "    test_array[idx] = fun_test(x)\n",
    "\n",
    "test_array"
   ]
  },
  {
   "source": [
    "### Further Questions/ Tasks\n",
    "- [] softmax derivative\n",
    "- [] read softmax glm implementation again\n",
    "- [] check where class check happens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}