{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other test variables, which are self parameters\n",
    "n_total_ftr = 0\n",
    "target_values = [1,2,3,4,5,6,7,8,9,0]\n",
    "mu = np.ones((n_total_ftr, len(target_values))) * 0\n",
    "sigma = np.ones((n_total_ftr, len(target_values))) * 1\n",
    "penalty_s = 0.01\n",
    "penalty_r = 0.01\n",
    "epochs = 1\n",
    "lr_mu = 0.01\n",
    "lr_sigma = 0.01\n",
    "monte_carlo = 10000\n",
    "\n",
    "# maybe set param amount_classes from target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __help_softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __softmax(x,y): # needs self in model\n",
    "    \"\"\"\n",
    "        Update the distribution parameters mu and sigma by optimizing them in terms of the (log) likelihood.\n",
    "        Here we assume a Bernoulli distributed target variable. We use a Probit model as our base model.\n",
    "        This corresponds to the FIRES-GLM model in the paper.\n",
    "\n",
    "        :param x: (np.ndarray) Batch of observations (numeric values only, consider normalizing data for better results)\n",
    "        :param y: (np.ndarray) Batch of labels: type integer e.g. 1,2,3,4 usw\n",
    "     \"\"\"\n",
    "\n",
    "     for epoch in range(epochs): #changed to self.epoch in model\n",
    "         # Shuffle the observations\n",
    "         random_idx = np.random.permutation(len(y))\n",
    "         x = x[random_idx]\n",
    "         y = y[random_idx]\n",
    "         \n",
    "         # Iterative update of mu and sigma\n",
    "         try:\n",
    "             for k in range(len(target_values)): #loop over all classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "test = np.ones((5,4))\n",
    "test[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.08372127, -0.8469425 , -0.18256183,  0.46443678,  0.93265726])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "#for getting gaussnorm r ~ N(0,1)\n",
    "np.random.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get theta at time t and for all featurs for class c is number of feature\n",
    "theta_c = sigma[:,c] * np.random.randn(total_features) + mu[:,c]"
   ]
  }
 ]
}