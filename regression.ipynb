{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Regression Case\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other test variables, which are self parameters\n",
    "n_total_ftr = 0\n",
    "target_values = [1,2,3,4,5,6,7,8,9,0]\n",
    "mu = np.ones() * 0\n",
    "sigma = np.ones() * 1\n",
    "penalty_s = 0.01\n",
    "penalty_r = 0.01\n",
    "epochs = 1\n",
    "lr_mu = 0.01\n",
    "lr_sigma = 0.01\n",
    "monte_carlo = 5 #10000\n",
    "\n",
    "# create 3d array with all r for current observation \n",
    "# r^cl_j = r[l, j, c]\n",
    "#needs to be specified only if we got right model\n",
    "r = np.random.randn(monte_carlo, n_total_ftr)\n",
    "\n",
    "# maybe set param amount_classes from target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __regression(x,y): # needs self in model\n",
    "    \"\"\"\n",
    "        Update the distribution parameters mu and sigma by optimizing them in terms of the (log) likelihood.\n",
    "        Here we assume a Bernoulli distributed target variable. We use a Probit model as our base model.\n",
    "        This corresponds to the FIRES-GLM model in the paper.\n",
    "\n",
    "        :param x: (np.ndarray) Batch of observations (numeric values only, consider normalizing data for better results)\n",
    "        :param y: (np.ndarray) Batch of labels: type integer e.g. 1,2,3,4 usw\n",
    "     \"\"\"\n",
    "\n",
    "     for epoch in range(epochs): #changed to self.epoch in model\n",
    "         # Shuffle the observations\n",
    "         random_idx = np.random.permutation(len(y))\n",
    "         x = x[random_idx]\n",
    "         y = y[random_idx]\n",
    "\n",
    "         #maybe loop over all given observations\n",
    "         x = x[0]\n",
    "         y = y[0]\n",
    "         # Iterative update of mu and sigma\n",
    "         try:\n",
    "             print(r.shape)\n",
    "             # calculate thetas for all samples and classes theta^cl_jt = theta[l,j,c]\n",
    "             theta = r * sigma + mu \n",
    "             \n",
    "             # calculate marginal likelihood\n",
    "             marginal = sum(np.einsum(\"ij->i\"), theta * x) / monte_carlo\n",
    "             \n",
    "             # calculate derivatives\n",
    "             nabla_mu = x\n",
    "             nabla_sigma = x * np.einsum(\"ij->i\") / monte_carlo\n",
    "\n",
    "             mu += lr_mu * nabla_mu / marginal\n",
    "             sigma += lr_sigma * nabla_sigma / marginal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.2694132   1.74360548 -0.81616493 -0.95192273 -1.153926  ]\n [ 0.61750079 -1.50378716 -1.26957848  0.50592964  0.56013337]\n [ 1.91835193  0.17786443 -0.41306479  0.96489759  1.73160213]\n [-1.38518285 -1.15311577 -0.1431769   0.1605285  -0.59358681]\n [ 0.68371675 -0.34167393 -0.21781114 -1.31752758  0.83507045]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.4611736 ,  4.48721095, -0.63232986, -0.90384547, -1.307852  ],\n",
       "       [ 2.23500158, -2.00757433, -1.53915695,  2.01185928,  2.12026673],\n",
       "       [ 4.83670385,  1.35572886,  0.17387042,  2.92979518,  4.46320427],\n",
       "       [-1.7703657 , -1.30623154,  0.7136462 ,  1.321057  , -0.18717363],\n",
       "       [ 2.3674335 ,  0.31665215,  0.56437771, -1.63505517,  2.67014089]])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "#test arrangment r, sigma mu\n",
    "sigma = np.ones(5) * 2\n",
    "mu = np.ones(5)\n",
    "r = np.random.randn(5,5)\n",
    "print(r)\n",
    "r*sigma+mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1 2 3]\n [1 2 3]\n [1 2 3]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([6, 6, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# mlh calc\n",
    "test1 = np.array([[1,2,3], [1,2,3], [1,2,3]])\n",
    "print(test1)\n",
    "np.einsum(\"ij->i\", test1) # for nabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}