{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = cp.zeros((784, 10))\n",
    "sigma = cp.ones((784,10))\n",
    "epochs = 1\n",
    "n_mc_samples = 100\n",
    "n_total_ftr = 784\n",
    "lr_mu = 1\n",
    "lr_sigma = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r = None\n",
    "test_theta = None\n",
    "test_eta_1 = None\n",
    "test_eta_2 = None\n",
    "test_eta = None\n",
    "test_eta_sum = None\n",
    "test_x_eta = None\n",
    "test_softmax = None\n",
    "test_obs_eta = None\n",
    "test_derivative = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_d(theta):\n",
    "    factor = -cp.floor(cp.log10(cp.max(theta)))\n",
    "    return(10**factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-05c7cf909b12>, line 80)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-05c7cf909b12>\"\u001b[0;36m, line \u001b[0;32m80\u001b[0m\n\u001b[0;31m    x_eta shape: oxlxj\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "    def __softmax(x, y):\n",
    "        global test_r \n",
    "        global test_theta\n",
    "        global test_eta_1\n",
    "        global test_eta_2\n",
    "        global test_eta\n",
    "        global test_derivative\n",
    "        global test_eta_sum\n",
    "        global test_x_eta\n",
    "        global test_obs_eta\n",
    "        global test_softmax\n",
    "        \"\"\"\n",
    "        Update the distribution parameters mu and sigma by optimizing them in terms of the (log) likelihood.\n",
    "        Here we assume a multinominal distributed target variable. We use a Multinominal model as our base model.\n",
    "\n",
    "\n",
    "        :param x: (np.ndarray) Batch of observations (numeric values only, consider normalizing data for better results)\n",
    "        :param y: (np.ndarray) Batch of labels: type integer e.g. 0,1,2,3,4 etc.\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(x.shape) != 2:\n",
    "            x = x.reshape(1,len(x))\n",
    "    \n",
    "        observed_classes = np.unique(y)\n",
    "\n",
    "        for obs_class in observed_classes:\n",
    "            observations_index = np.where(y == obs_class)[0]\n",
    "            x_obs = cp.array(x[observations_index])\n",
    "            print(\"x_obs: {}\".format(cp.mean(cp.isinf(x_obs))))\n",
    "            n_obs = len(x_obs)\n",
    "            print(\"obs_class: {}, n obs: {}\".format(obs_class, n_obs))\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                    \n",
    "                    # Iterative update of mu and sigma\n",
    "                    try:\n",
    "                        # o number of obs, l number of samples, j features,\n",
    "                        # c classes\n",
    "                        \n",
    "                        # r shape: oxlxjxc\n",
    "                        r = cp.random.randn(n_obs, n_mc_samples,\n",
    "                                            n_total_ftr,\n",
    "                                            10)\n",
    "                        print(\"r: {}\".format(cp.mean(cp.isinf(r))))\n",
    "                        # theta shape: oxlxjxc\n",
    "                        theta = (r * sigma + mu)\n",
    "                        d = 0.1\n",
    "                        print(d)\n",
    "                        \n",
    "\n",
    "                        print(\"theta: {}\".format(cp.mean(cp.isinf(theta))))\n",
    "                        # eta shape: oxlxc\n",
    "                        # multiply all ftr_cols with given ftr_vector x\n",
    "                        eta = d * cp.einsum(\"oljc,oj->oljc\", theta, x_obs) \n",
    "                        test_eta_1 = eta\n",
    "                        # sum up all theta^cl_j * x_tj so we got l samples\n",
    "                        # for all c classes\n",
    "                        eta = cp.einsum(\"oljc->olc\", eta) \n",
    "                        test_eta_2 = eta\n",
    "                        eta = cp.exp(eta) # we only need them exp\n",
    "                        print(\"eta: {}\".format(cp.mean(cp.isinf(eta))))\n",
    "\n",
    "                        # eta_sum shape: oxl\n",
    "                        eta_sum = cp.einsum(\"olc->ol\", eta)\n",
    "                        print(\"eta_sum: {}\".format(cp.mean(cp.isinf(eta_sum))))\n",
    "                        # calculate softmax only for observed class\n",
    "                        # obs_eta shape: oxl\n",
    "                        obs_eta = eta[:,:,obs_class]\n",
    "                        print(\"obs_eta: {}\".format(cp.mean(cp.isinf(obs_eta))))\n",
    "                        # softmax_lh shape: oxl\n",
    "                        softmax_lh = obs_eta / eta_sum # \n",
    "                        print(\"softmax: {}\".format(cp.mean(cp.isinf(softmax_lh))))\n",
    "                        # marginal shape: o\n",
    "                        marginal = cp.einsum(\"ol->o\", softmax_lh) / \\\n",
    "                                   n_mc_samples\n",
    "                        print(\"marginal: {}\".format(cp.mean(cp.isinf(marginal))))\n",
    "\n",
    "                        # calculate softmax derivative to theta\n",
    "                        \n",
    "                        x_eta shape: oxlxj\n",
    "                        x_eta = d*cp.einsum(\"oj,ol->olj\", x_obs, obs_eta)\n",
    "                        \n",
    "                        print(\"x_eta: {}\".format(cp.mean(cp.isinf(x_eta))))\n",
    "                        softmax_derivative = cp.einsum(\"olj,ol->olj\", x_eta,\n",
    "                                                       (eta_sum - obs_eta))\n",
    "                        print(\"derivative1: {}\".format(cp.mean(cp.isinf                                                     x                 (softmax_derivative))))                               \n",
    "                        softmax_derivative = cp.einsum(\"olj->jol\", softmax_derivative)\n",
    "                        print(\"derivative2: {}\".format(cp.mean(cp.isinf                                                                       (softmax_derivative))))\n",
    "                        holder = softmax_derivative                     \n",
    "                        print(\"eta_sum^2: {}\".format(cp.mean(cp.isinf(eta_sum**2))))\n",
    "                        softmax_derivative = cp.divide(softmax_derivative, eta_sum**2)\n",
    "                        print(\"derivative3: {}\".format(cp.mean(cp.isinf( softmax_derivative))))\n",
    "\n",
    "                        # with new def and d\n",
    "\n",
    "                        # softmax_derivative = d * x_obs * softmax_lh * (1 - softmax_lh)\n",
    "                        #softmax_derivative = cp.einsum(\"oj,ol->olj\", (d*x_obs),                               #                                softmax_lh)\n",
    "                        #softmax_derivative = cp.einsum(\"olj,ol->olj\",                                         #                                softmax_derivative,                                                                    (1-softmax_lh))\n",
    "\n",
    "                        test_r = r\n",
    "                        test_theta = theta\n",
    "                        test_eta = eta\n",
    "                        test_eta_sum = eta_sum\n",
    "                        test_softmax = softmax_lh\n",
    "                        #test_x_eta = x_eta\n",
    "                        test_obs_eta = obs_eta\n",
    "                        test_derivative = softmax_derivative\n",
    "                        \n",
    "                    \n",
    "                        #softmax_derivative = cp.einsum(\"jol->olj\",\n",
    "                         #                              softmax_derivative)\n",
    "\n",
    "                                                       \n",
    "                        print(\"derivative4: {}\".format(cp.mean(cp.isinf(softmax_derivative))))\n",
    "                        nabla_mu = cp.einsum(\"olj->oj\", softmax_derivative) / \\\n",
    "                                   n_mc_samples\n",
    "                                                \n",
    "                        print(\"nabla_mu: {}\".format(cp.mean(cp.isinf(nabla_mu))))\n",
    "                        r_jc = r[:,:,:,obs_class]\n",
    "                        print(r_jc.shape)\n",
    "                        nabla_sigma = cp.einsum(\"olj->oj\",\n",
    "                                                softmax_derivative * r_jc) / \\\n",
    "                                      n_mc_samples\n",
    "                        print(\"nabla_mu: {}\".format(cp.mean(cp.isinf(nabla_sigma))))\n",
    "\n",
    "                        mu[:,obs_class] += lr_mu * \\\n",
    "                                                cp.einsum(\"jo->j\",\n",
    "                                                          (nabla_mu.T / marginal))\n",
    "                        print(\"mu: {}\".format(cp.mean(cp.isinf(mu))))\n",
    "                        sigma[:,obs_class] += lr_sigma * \\\n",
    "                                                   cp.einsum(\"jo->j\",\n",
    "                                                             (nabla_sigma.T / marginal))\n",
    "                        print(\"sigma: {}\".format(cp.mean(cp.isinf(sigma))))\n",
    "                        \n",
    "                    \n",
    "                    except TypeError as e:\n",
    "                            raise TypeError('All features must be a numeric data type.') from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultiflow.data import FileStream\n",
    "stream = FileStream('../datasets/Multiclass/mnist_train_normalized.csv', target_idx=0)\n",
    "stream.prepare_for_use()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_obs: 0.0\n",
      "obs_class: 0, n obs: 5\n",
      "r: 0.0\n",
      "0.1\n",
      "theta: 0.0\n",
      "eta: 0.0\n",
      "eta_sum: 0.0\n",
      "obs_eta: 0.0\n",
      "softmax: 0.0\n",
      "marginal: 0.0\n",
      "derivative4: 0.0\n",
      "nabla_mu: 0.0\n",
      "(5, 100, 784)\n",
      "nabla_mu: 0.0\n",
      "mu: 0.0\n",
      "sigma: 0.0\n",
      "x_obs: 0.0\n",
      "obs_class: 1, n obs: 13\n",
      "r: 0.0\n",
      "0.1\n",
      "theta: 0.0\n",
      "eta: 0.0\n",
      "eta_sum: 0.0\n",
      "obs_eta: 0.0\n",
      "softmax: 0.0\n",
      "marginal: 0.0\n",
      "derivative4: 0.0\n",
      "nabla_mu: 0.0\n",
      "(13, 100, 784)\n",
      "nabla_mu: 0.0\n",
      "mu: 0.0\n",
      "sigma: 0.0\n",
      "x_obs: 0.0\n",
      "obs_class: 2, n obs: 15\n",
      "r: 0.0\n",
      "0.1\n",
      "theta: 0.0\n",
      "eta: 0.0\n",
      "eta_sum: 0.0\n",
      "obs_eta: 0.0\n",
      "softmax: 0.0\n",
      "marginal: 0.0\n",
      "derivative4: 0.0\n",
      "nabla_mu: 0.0\n",
      "(15, 100, 784)\n",
      "nabla_mu: 0.0\n",
      "mu: 0.0\n",
      "sigma: 0.0\n",
      "x_obs: 0.0\n",
      "obs_class: 3, n obs: 7\n",
      "r: 0.0\n",
      "0.1\n",
      "theta: 0.0\n",
      "eta: 0.0\n",
      "eta_sum: 0.0\n",
      "obs_eta: 0.0\n",
      "softmax: 0.0\n",
      "marginal: 0.0\n",
      "derivative4: 0.0\n",
      "nabla_mu: 0.0\n",
      "(7, 100, 784)\n",
      "nabla_mu: 0.0\n",
      "mu: 0.0\n",
      "sigma: 0.0\n",
      "x_obs: 0.0\n",
      "obs_class: 4, n obs: 12\n",
      "r: 0.0\n",
      "0.1\n",
      "theta: 0.0\n",
      "eta: 0.0\n",
      "eta_sum: 0.0\n",
      "obs_eta: 0.0\n",
      "softmax: 0.0\n",
      "marginal: 0.0\n",
      "derivative4: 0.0\n",
      "nabla_mu: 0.0\n",
      "(12, 100, 784)\n",
      "nabla_mu: 0.0\n",
      "mu: 0.0\n",
      "sigma: 0.0\n",
      "x_obs: 0.0\n",
      "obs_class: 5, n obs: 7\n",
      "r: 0.0\n",
      "0.1\n",
      "theta: 0.0\n",
      "eta: 0.0\n",
      "eta_sum: 0.0\n",
      "obs_eta: 0.0\n",
      "softmax: 0.0\n",
      "marginal: 0.0\n",
      "derivative4: 0.0\n",
      "nabla_mu: 0.0\n",
      "(7, 100, 784)\n",
      "nabla_mu: 0.0\n",
      "mu: 0.0\n",
      "sigma: 0.0\n",
      "x_obs: 0.0\n",
      "obs_class: 6, n obs: 8\n",
      "r: 0.0\n",
      "0.1\n",
      "theta: 0.0\n",
      "eta: 0.0\n",
      "eta_sum: 0.0\n",
      "obs_eta: 0.0\n",
      "softmax: 0.0\n",
      "marginal: 0.0\n",
      "derivative4: 0.0\n",
      "nabla_mu: 0.0\n",
      "(8, 100, 784)\n",
      "nabla_mu: 0.0\n",
      "mu: 0.0\n",
      "sigma: 0.0\n",
      "x_obs: 0.0\n",
      "obs_class: 7, n obs: 12\n",
      "r: 0.0\n",
      "0.1\n",
      "theta: 0.0\n",
      "eta: 0.0\n",
      "eta_sum: 0.0\n",
      "obs_eta: 0.0\n",
      "softmax: 0.0\n",
      "marginal: 0.0\n",
      "derivative4: 0.0\n",
      "nabla_mu: 0.0\n",
      "(12, 100, 784)\n",
      "nabla_mu: 0.0\n",
      "mu: 0.0\n",
      "sigma: 0.0\n",
      "x_obs: 0.0\n",
      "obs_class: 8, n obs: 8\n",
      "r: 0.0\n",
      "0.1\n",
      "theta: 0.0\n",
      "eta: 0.0\n",
      "eta_sum: 0.0\n",
      "obs_eta: 0.0\n",
      "softmax: 0.0\n",
      "marginal: 0.0\n",
      "derivative4: 0.0\n",
      "nabla_mu: 0.0\n",
      "(8, 100, 784)\n",
      "nabla_mu: 0.0\n",
      "mu: 0.0\n",
      "sigma: 0.0\n",
      "x_obs: 0.0\n",
      "obs_class: 9, n obs: 13\n",
      "r: 0.0\n",
      "0.1\n",
      "theta: 0.0\n",
      "eta: 0.0\n",
      "eta_sum: 0.0\n",
      "obs_eta: 0.0\n",
      "softmax: 0.0\n",
      "marginal: 0.0\n",
      "derivative4: 0.0\n",
      "nabla_mu: 0.0\n",
      "(13, 100, 784)\n",
      "nabla_mu: 0.0\n",
      "mu: 0.0\n",
      "sigma: 0.0\n",
      "0.2496993400000065\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.03565111, 0.08775658, 0.08627545, ..., 0.00548781, 0.087413  ,\n",
       "       0.04311852])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "x,y = stream.next_sample(batch_size=100)\n",
    "\n",
    "start = timer()\n",
    "__softmax(x,y)\n",
    "print(timer()-start)\n",
    "mu[mu!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stream.restart()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infs(arr):\n",
    "    return(cp.mean(cp.isinf(arr)))\n",
    "\n",
    "def nans(arr):\n",
    "    return(cp.mean(cp.isnan(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[0,1,0,0,1],[1,0,1,0,0],[1,0,0,0,0],[1,0,1,1,1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v (arr):\n",
    "    k = np.sum(arr)\n",
    "    d = len(arr)\n",
    "    return np.sqrt((k/d)*(1-k/d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n3\n4\n5\n0.4898979485566356\n0.39999999999999997\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-2.4494897427831783"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "pearson_stability(arr, arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_stability_ij(arr1,arr2):\n",
    "    d = len(arr1)\n",
    "    k_i = np.sum(arr1)\n",
    "    k_j = np.sum(arr2)\n",
    "    x_hat_i = k_i / d\n",
    "    x_hat_j = k_j / d\n",
    "    arr1 = arr1 - x_hat_i\n",
    "    arr2 = arr2 - x_hat_j\n",
    "    dividend = 1/d * np.sum(arr1*arr2)\n",
    "    divisor = np.sqrt(1/d*np.sum(arr1**2))*np.sqrt(1/d*np.sum(arr2**2))\n",
    "    return dividend/divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability_factor(selected_ftrs):\n",
    "    M = len(selected_ftrs)\n",
    "    sum_stabilities = 0\n",
    "    for i in range(M):\n",
    "        print(i)\n",
    "        for j in range(i+1, M):\n",
    "            print(\"{}{}\".format(i,j))\n",
    "            print(pearson_stability_ij(selected_ftrs[i], selected_ftrs[j]))\n",
    "            sum_stabilities += pearson_stability_ij(selected_ftrs[i], selected_ftrs[j])\n",
    "    return 1/(M*(M-1))*sum_stabilities * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n01\n-0.6666666666666665\n02\n-0.40824829046386296\n03\n-0.6123724356957942\n1\n12\n0.6123724356957945\n13\n0.408248290463863\n2\n23\n0.25\n3\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.06944444444444436"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "stability_factor(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4,4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(2,5)"
   ]
  }
 ]
}