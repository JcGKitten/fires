{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(([1,2,3],[4,5,6]))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cp.array(x)\n",
    "type(x_gpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu * x_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = cp.ones((784,10))\n",
    "mu = cp.ones((784,10))\n",
    "n_mc_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __softmax(x,y): # needs self in model\n",
    "    # sort observations\n",
    "    print(\"{} {}\".format(\"len(y)\", len(x.shape)))\n",
    "    if len(x.shape) != 2:\n",
    "        # if only one observation array is given, reshape it\n",
    "        # only one ftr should not happen...\n",
    "        x = x.reshape(1,len(x))\n",
    " \n",
    "    observed_classes = np.unique(y)\n",
    "\n",
    "    for obs_class in observed_classes:\n",
    "        # fix access\n",
    "        print(\"obs_class: {}\".format(obs_class))\n",
    "        observations_index = np.where(y == obs_class)[0]\n",
    "        print(observations_index)\n",
    "        x_obs = x[observations_index]\n",
    "        print(x_obs)\n",
    "        n_obs = len(x_obs)\n",
    "        print(n_obs)\n",
    "        x_obs = cp.array(x_obs)\n",
    "    \n",
    "        for epoch in range(epochs): #changed to self.epoch in model\n",
    "                \n",
    "                # Iterative update of mu and sigma\n",
    "                try:\n",
    "                    # o number of obs, l number of samples, j features, c classes\n",
    "                    # create 3d array with all r for current observation for multiple observation calculation we would need 4d array\n",
    "                    # r^cl_j = r[l, j, c] lxjxc\n",
    "                    r = cp.random.randn(n_obs, no_mc_samples, n_total_ftr, len(target_values))\n",
    "                    #r = np.array([[[-0.558, 1.555], [0.325, -0.726], [0.347,-0.159]],\n",
    "                    #              [[-0.955, 0.283], [0.115,-1.637], [-0.516,0.161]]])  \n",
    "                    print(r)\n",
    "                    # we only change the psi for the actuall given class still need all classes of course\n",
    "                    # r = np.random.randn(monte_carlo, n_total_ftr)\n",
    "\n",
    "                    print(r.shape)\n",
    "                    # calculate thetas for all samples and classes theta^cl_jt = theta[l,j,c]\n",
    "                    # oxlxjxc\n",
    "                    theta = r * sigma + mu\n",
    "                    print(theta.shape)\n",
    "                    print(theta)\n",
    "                    #calculate all the etas\n",
    "                    eta = cp.einsum(\"oljc,oj->oljc\", theta, x_obs) # multiply all ftr_cols with given ftr_vector x\n",
    "                    print('theta * x:')\n",
    "                    print(eta)\n",
    "                    eta = cp.einsum(\"oljc->olc\", eta) #sum up all theta^cl_j * x_tj so we got l samples for all c classes\n",
    "                    print(\"eta:\")\n",
    "                    print(eta)\n",
    "                    eta = cp.exp(eta) # we only need them exp\n",
    "                    print(\"eta_exp:\")\n",
    "                    print(eta)\n",
    "                    print(eta.shape)\n",
    "                    eta_sum = cp.einsum(\"olc->ol\", eta) #sum up etas for the l samples\n",
    "                    print(\"eta_sum:\")\n",
    "                    print(eta_sum)\n",
    "                    print(eta_sum.shape)\n",
    "                    #calculate softmax only for observed class\n",
    "                    #observation_etas = eta[:,y] with more observations we need transposition\n",
    "                    obs_eta = eta[:,:,obs_class]\n",
    "                    print(\"Obs_eta shape: {}\".format(obs_eta.shape))\n",
    "                    softmax_lh = obs_eta / eta_sum # \n",
    "                    print(softmax_lh)\n",
    "                    print(softmax_lh.shape) #should be oxl\n",
    "                    #marginal = np.einsum(\"lo->o\", softmax_lh) / monte_carlo # 1xy\n",
    "                    marginal = cp.einsum(\"ol->o\", softmax_lh) / no_mc_samples\n",
    "                    print(marginal.shape) #should be o\n",
    "                    print(marginal)\n",
    "                    # calculate derivatives nabla_mu, nabla_sigma must be handled better\n",
    "\n",
    "                    #first calculate softmax dtheta\n",
    "                    # x_eta means observations x times the beloning etas\n",
    "                    x_eta = cp.einsum(\"oj,ol->olj\", x_obs, obs_eta)\n",
    "                    print(x_eta)\n",
    "\n",
    "                    softmax_derivative = cp.einsum(\"olj,ol->olj\", x_eta, (eta_sum - obs_eta))\n",
    "                    softmax_derivative = cp.einsum(\"olj->jol\", softmax_derivative) /  eta_sum**2\n",
    "                    softmax_derivative = cp.einsum(\"jol->olj\", softmax_derivative)\n",
    "                    print(softmax_derivative.shape)\n",
    "                    print(softmax_derivative)\n",
    "                    nabla_mu = cp.einsum(\"olj->oj\", softmax_derivative) / no_mc_samples\n",
    "                    print(nabla_mu.shape) #oj\n",
    "                    print(nabla_mu)\n",
    "                    r_jc = r[:,:,:,obs_class]\n",
    "                    print(r_jc.shape)\n",
    "                    nabla_sigma = cp.einsum(\"olj->oj\", softmax_derivative * r_jc) / no_mc_samples\n",
    "                    print(nabla_sigma.shape) #oj\n",
    "                    # Update parameters\n",
    "                    mu[:,obs_class] += lr_mu * cp.einsum(\"jo->j\", (nabla_mu.T / marginal))\n",
    "                    sigma[:,obs_class] += lr_sigma * cp.einsum(\"jo->j\",(nabla_sigma.T / marginal))\n",
    "                    \n",
    "\n",
    "                except TypeError as e:\n",
    "                        raise TypeError('All features must be a numeric data type.') from e"
   ]
  }
 ]
}