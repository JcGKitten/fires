{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Multiclass Case"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other test variables, which are self parameters\n",
    "# classes schould be given as int from 0 to k\n",
    "n_total_ftr = 3\n",
    "target_values = [0,1]\n",
    "mu = cp.ones((n_total_ftr, len(target_values))) * 0\n",
    "sigma = cp.ones((n_total_ftr, len(target_values))) * 1\n",
    "penalty_s = 0.01\n",
    "penalty_r = 0.01\n",
    "epochs = 1\n",
    "lr_mu = 1\n",
    "lr_sigma = 1\n",
    "n_mc_samples = 2 # monte carlo samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __softmax( x, y):\n",
    "\n",
    "        \"\"\"\n",
    "        Update the distribution parameters mu and sigma by optimizing them in terms of the (log) likelihood.\n",
    "        Here we assume a multinominal distributed target variable. We use a Multinominal model as our base model.\n",
    "        Funciton with cupy functions.\n",
    "\n",
    "        :param x: (np.ndarray) Batch of observations (numeric values only, consider normalizing data for better results)\n",
    "        :param y: (np.ndarray) Batch of labels: type integer e.g. 0,1,2,3,4 etc.\n",
    "        \"\"\"\n",
    "        global sigma\n",
    "        global mu\n",
    "        global epochs\n",
    "        global lr_mu\n",
    "        global lr_sigma\n",
    "        global n_mc_samples\n",
    "        if len(x.shape) != 2:\n",
    "            x = x.reshape(1,len(x))\n",
    "    \n",
    "        observed_classes = np.unique(y)\n",
    "\n",
    "        for obs_class in observed_classes:\n",
    "            observations_index = np.where(y == obs_class)[0]\n",
    "            x_obs = cp.array(x[observations_index])\n",
    "            print(\"x:\")\n",
    "            print(x_obs)\n",
    "            n_obs = len(x_obs)\n",
    "            print(\"y: {}\".format(obs_class))\n",
    "            #print(\"obs_class: {}, n obs: {}\".format(obs_class, n_obs))\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                    \n",
    "                    # Iterative update of mu and sigma\n",
    "                    try:\n",
    "                        # o number of obs, l number of samples, j features,\n",
    "                        # c classes\n",
    "                        \n",
    "                        # r shape: oxlxjxc\n",
    "                        r = cp.random.randn(n_obs, n_mc_samples,\n",
    "                                            n_total_ftr,\n",
    "                                            len(target_values))\n",
    "\n",
    "                        r[0,:,:,:] = cp.array(([[0.5,0.5],[0.5,0.5],[0.5,0.5]],[[1,1],[1,1],[1,1]]))\n",
    "                        r.reshape(1,2,3,2)\n",
    "                        print(r)\n",
    "                        \n",
    "\n",
    "                        # theta shape: oxlxjxc\n",
    "                        theta = (r * sigma + mu)\n",
    "                        print(\"theta:{}\".format(theta.shape))\n",
    "                        # eta shape: oxlxc\n",
    "                        # multiply all ftr_cols with given ftr_vector x\n",
    "                        print(theta)\n",
    "                        eta = cp.einsum(\"oljc,oj->oljc\", theta, x_obs) \n",
    "\n",
    "                        # sum up all theta^cl_j * x_tj so we got l samples\n",
    "                        # for all c classes\n",
    "                        eta = cp.einsum(\"oljc->olc\", eta) \n",
    "                        print(\"eta: {}\".format(eta.shape))\n",
    "                        print(eta)\n",
    "\n",
    "                        # get a for numerical stability, shape oxl\n",
    "                        a = cp.amax(eta, axis=2) * -1\n",
    "                        print(\"a: {}\".format(a.shape))\n",
    "                        print(a)\n",
    "\n",
    "                        eta = cp.einsum(\"olc->col\", eta) + a\n",
    "                        eta = cp.einsum(\"col->olc\", eta)\n",
    "                        print(\"eta with a: {}\".format(eta.shape))\n",
    "                        print(eta)\n",
    "\n",
    "\n",
    "                        eta = cp.exp(eta) # we only need them exp\n",
    "                        print(\"finally eta: {}\".format(eta.shape))\n",
    "                        print(eta)\n",
    "\n",
    "                        # eta_sum shape: oxl\n",
    "                        eta_sum = cp.einsum(\"olc->ol\", eta)\n",
    "                        print(\"sums:{}\".format(eta_sum.shape))\n",
    "                        print(eta_sum)\n",
    "                        \n",
    "                        # calculate softmax only for all classes\n",
    "                        # divide all etas by eta_sum\n",
    "                        softmax_all = np.einsum(\"olc,ol->olc\", eta, (1/eta_sum))\n",
    "\n",
    "                        print(\"softmax: {}\".format(softmax_all.shape))\n",
    "                        \n",
    "                        # marginal shape: o\n",
    "                        marginal = cp.einsum(\"ol->o\",\n",
    "                                             softmax_all[:,:,obs_class]) / \\\n",
    "                                   n_mc_samples\n",
    "\n",
    "                        print(\"marginal: {}\".format(marginal.shape))\n",
    "                        print(marginal)\n",
    "\n",
    "\n",
    "                        # calculate softmax derivative to theta\n",
    "                        softmax_c = softmax_all[:,:,obs_class]\n",
    "                        print(\"sm_c:{}\".format(softmax_c.shape))\n",
    "\n",
    "                        # first calculate derivative for all as k != c\n",
    "                        softmax_derivative = -1 * cp.einsum(\"oj,ol,olc->oljc\",\n",
    "                                                            (x_obs),\n",
    "                                                            softmax_c,\n",
    "                                                            softmax_all)\n",
    "                        print(\"derivavtive: {}\".format(softmax_derivative.shape))\n",
    "                        print(softmax_derivative)\n",
    "\n",
    "                        # then for observed class c\n",
    "                        softmax_derivative_c = cp.einsum(\"oj,ol,ol->olj\",\n",
    "                                                         x_obs,\n",
    "                                                         softmax_c,\n",
    "                                                         (1-softmax_c))\n",
    "                        print(\"derivative c: {}\".format(softmax_derivative_c.shape))\n",
    "                        print(softmax_derivative_c)\n",
    "\n",
    "                        softmax_derivative[:,:,:,obs_class] = \\\n",
    "                            softmax_derivative_c\n",
    "                        \n",
    "                        print(\"end derivative: {}\".format(softmax_derivative.shape))\n",
    "                        print(softmax_derivative)\n",
    "\n",
    "                        nabla_mu = cp.einsum(\"oljc->ojc\", softmax_derivative) /\\\n",
    "                                   n_mc_samples\n",
    "\n",
    "                        print(\"nabla_mu: {}\".format(nabla_mu.shape))\n",
    "                        print(nabla_mu)\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        nabla_sigma = cp.einsum(\"oljc,oljc->ojc\",\n",
    "                                                softmax_derivative,r) / \\\n",
    "                                      n_mc_samples\n",
    "                        print(\"nablu_sigma: {}\".format(nabla_sigma.shape))\n",
    "                        print(nabla_sigma)\n",
    "                                    \n",
    "\n",
    "                        nabla_mu = cp.einsum(\"ojc->jco\", nabla_mu)\n",
    "                        mu += lr_mu * \\\n",
    "                                                cp.einsum(\"jco->jc\",\n",
    "                                                          (nabla_mu/ marginal))\n",
    "                        print(\"mu:\")\n",
    "                        print(mu)\n",
    "\n",
    "                        nabla_sigma = cp.einsum(\"ojc->jco\", nabla_sigma)\n",
    "                        sigma += lr_sigma * \\\n",
    "                                                   cp.einsum(\"jco->jc\",\n",
    "                                                             (nabla_sigma / \n",
    "                                                             marginal))\n",
    "                        print(\"sigma:\")\n",
    "                        print(sigma)\n",
    "\n",
    "                    except TypeError as e:\n",
    "                            raise TypeError('All features must be a numeric data type.') from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x:\n",
      "[[1 2 3]]\n",
      "y: 1\n",
      "[[[[0.5 0.5]\n",
      "   [0.5 0.5]\n",
      "   [0.5 0.5]]\n",
      "\n",
      "  [[1.  1. ]\n",
      "   [1.  1. ]\n",
      "   [1.  1. ]]]]\n",
      "theta:(1, 2, 3, 2)\n",
      "[[[[0.5 0.5]\n",
      "   [0.5 0.5]\n",
      "   [0.5 0.5]]\n",
      "\n",
      "  [[1.  1. ]\n",
      "   [1.  1. ]\n",
      "   [1.  1. ]]]]\n",
      "eta: (1, 2, 2)\n",
      "[[[3. 3.]\n",
      "  [6. 6.]]]\n",
      "a: (1, 2)\n",
      "[[-3. -6.]]\n",
      "eta with a: (1, 2, 2)\n",
      "[[[0. 0.]\n",
      "  [0. 0.]]]\n",
      "finally eta: (1, 2, 2)\n",
      "[[[1. 1.]\n",
      "  [1. 1.]]]\n",
      "sums:(1, 2)\n",
      "[[2. 2.]]\n",
      "softmax: (1, 2, 2)\n",
      "marginal: (1,)\n",
      "[0.5]\n",
      "sm_c:(1, 2)\n",
      "derivavtive: (1, 2, 3, 2)\n",
      "[[[[-0.25 -0.25]\n",
      "   [-0.5  -0.5 ]\n",
      "   [-0.75 -0.75]]\n",
      "\n",
      "  [[-0.25 -0.25]\n",
      "   [-0.5  -0.5 ]\n",
      "   [-0.75 -0.75]]]]\n",
      "derivative c: (1, 2, 3)\n",
      "[[[0.25 0.5  0.75]\n",
      "  [0.25 0.5  0.75]]]\n",
      "end derivative: (1, 2, 3, 2)\n",
      "[[[[-0.25  0.25]\n",
      "   [-0.5   0.5 ]\n",
      "   [-0.75  0.75]]\n",
      "\n",
      "  [[-0.25  0.25]\n",
      "   [-0.5   0.5 ]\n",
      "   [-0.75  0.75]]]]\n",
      "nabla_mu: (1, 3, 2)\n",
      "[[[-0.25  0.25]\n",
      "  [-0.5   0.5 ]\n",
      "  [-0.75  0.75]]]\n",
      "nablu_sigma: (1, 3, 2)\n",
      "[[[-0.1875  0.1875]\n",
      "  [-0.375   0.375 ]\n",
      "  [-0.5625  0.5625]]]\n",
      "mu:\n",
      "[[-0.5  0.5]\n",
      " [-1.   1. ]\n",
      " [-1.5  1.5]]\n",
      "sigma:\n",
      "[[ 0.625  1.375]\n",
      " [ 0.25   1.75 ]\n",
      " [-0.125  2.125]]\n"
     ]
    }
   ],
   "source": [
    "#test runs\n",
    "__softmax(np.array((1,2,3)),1)"
   ]
  }
 ]
}